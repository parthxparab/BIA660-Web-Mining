{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster import KMeansClusterer, \\\n",
    "cosine_distance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1\n",
      "cosine\n",
      "actual_class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                71                 5                      157\n",
      "1                               129                 6                       18\n",
      "2                                10               195                        9\n",
      "Cluster 0 : Topic Travel & Transportation\n",
      "Cluster 1 : Topic Disaster and Accident\n",
      "Cluster 2 : Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.84      0.61      0.71       210\n",
      "       News and Economy       0.91      0.95      0.93       206\n",
      "Travel & Transportation       0.67      0.85      0.75       184\n",
      "\n",
      "               accuracy                           0.80       600\n",
      "              macro avg       0.81      0.80      0.80       600\n",
      "           weighted avg       0.81      0.80      0.80       600\n",
      "\n",
      "L2\n",
      "actual_class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                               132                53                      180\n",
      "1                                10               152                        4\n",
      "2                                68                 1                        0\n",
      "Cluster 0 : Topic Disaster and Accident\n",
      "Cluster 0 : Topic Travel & Transportation\n",
      "Cluster 1 : Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.36      0.63      0.46       210\n",
      "       News and Economy       0.01      0.00      0.01       206\n",
      "Travel & Transportation       0.02      0.02      0.02       184\n",
      "\n",
      "               accuracy                           0.23       600\n",
      "              macro avg       0.13      0.22      0.16       600\n",
      "           weighted avg       0.14      0.23      0.17       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "def cluster_kmean(train_file, test_file):\n",
    "    # add your code here\n",
    "    train=pd.read_json(train_file)\n",
    "    test=pd.read_json(test_file)\n",
    "    test_label = [test[1][x][0] for x in range(len(test))]\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(stop_words=\"english\",\\\n",
    "                             min_df=5) \n",
    "\n",
    "    dtm= tfidf_vect.fit_transform(train[0])\n",
    "    \n",
    "    num_clusters=3\n",
    "    \n",
    "    #### COSINE ####\n",
    "\n",
    "    clusterer = KMeansClusterer(num_clusters, \\\n",
    "                        cosine_distance, \\\n",
    "                            repeats=25)\n",
    "\n",
    "    clusters_cosine = clusterer.cluster(dtm.toarray(), \\\n",
    "                             assign_clusters=True)\n",
    "    test_dtm = tfidf_vect.transform(test[0])\n",
    "\n",
    "    predicted_cosine = [clusterer.classify(v) for v in test_dtm.toarray()]    \n",
    "    \n",
    "    confusion_df_cs = pd.DataFrame(list(zip(test_label, predicted_cosine)),\\\n",
    "                            columns = [\"actual_class\", \"cluster\"])\n",
    "\n",
    "    crosstab_cs = pd.crosstab( index=confusion_df_cs.cluster, columns=confusion_df_cs.actual_class)\n",
    "    cs_idx = crosstab_cs.idxmax(axis = 0)\n",
    "    cs_idx = cs_idx.sort_values(ascending=True) \n",
    "    \n",
    "    cluster_dict={0:'Travel & Transportation',\\\n",
    "              1:\"Disaster and Accident\",\\\n",
    "              2:'News and Economy'}\n",
    "    predicted_target=[cluster_dict[i] \\\n",
    "                  for i in predicted_cosine]\n",
    "    \n",
    "    print(\"cosine\")\n",
    "    print(crosstab_cs)\n",
    "    for index, val in cs_idx.iteritems():\n",
    "        print(\"Cluster\",val,\": Topic\",index)  \n",
    "    print(metrics.classification_report\\\n",
    "      (test_label, predicted_target))\n",
    "\n",
    "\n",
    "    ###### EUCLIDEAN ######\n",
    "    \n",
    "    km = KMeans(n_clusters=num_clusters, n_init=25).fit(dtm)\n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    test_dtm = tfidf_vect.transform(test[0])\n",
    "\n",
    "    predicted_euclidean = km.predict(test_dtm)\n",
    "    \n",
    "    confusion_df_eu = pd.DataFrame(list(zip(test_label, predicted_euclidean)),\\\n",
    "                            columns = [\"actual_class\", \"cluster\"])\n",
    "    crosstab_eu = pd.crosstab( index=confusion_df_eu.cluster, columns=confusion_df_eu.actual_class)\n",
    "    eu_idx = crosstab_eu.idxmax(axis = 0)\n",
    "    \n",
    "    eu_idx = eu_idx.sort_values(ascending=True)\n",
    "    cluster_dict={1:'Travel & Transportation',\\\n",
    "              0:\"Disaster and Accident\",\\\n",
    "              2:'News and Economy'}\n",
    "    predicted_target=[cluster_dict[i] \\\n",
    "                  for i in predicted_euclidean]\n",
    "\n",
    "    print(\"L2\")\n",
    "    print(crosstab_eu)\n",
    "    for index, val in eu_idx.iteritems():\n",
    "        print(\"Cluster\",val,\": Topic\",index)\n",
    "    print(metrics.classification_report\\\n",
    "      (test_label, predicted_target))\n",
    "\n",
    "\n",
    "    return None\n",
    "\n",
    "# Q1\n",
    "print(\"Q1\")\n",
    "cluster_kmean('/Users/parthxparab/Documents/Fall 2019/BIA660/Lab5/Assignment/train_text.json', \\\n",
    "             '/Users/parthxparab/Documents/Fall 2019/BIA660/Lab5/Assignment/test_text.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0, 1, 1, 1, 1, 0, 1, 2, 0, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 2, 1, 2, 2, 1, 0, 0, 0, 0, 2, 2, 0, 1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 1, 1, 2, 0, 2, 1, 1, 2, 0, 1, 1, 0, 1, 0, 2, 0, 1, 2, 0, 1, 2, 1, 0, 1, 1, 0, 1, 0, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 0, 1, 1, 2, 0, 0, 2, 1, 1, 2, 1, 2, 0, 1, 1, 0, 0, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 2, 2, 1, 0, 2, 0, 1, 1, 1, 2, 1, 1, 0, 2, 2, 0, 1, 1, 2, 1, 2, 0, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 2, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 2, 1, 1, 1, 1, 0, 2, 0, 1, 0, 2, 1, 2, 0, 1, 0, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 2, 1, 0, 0, 2, 2, 1, 1, 1, 1, 0, 1, 2, 2, 2, 1, 2, 0, 2, 0, 1, 0, 1, 1, 1, 1, 0, 0, 2, 2, 1, 1, 0, 2, 1, 2, 2, 2, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 2, 1, 1, 2, 1, 0, 2, 2, 1, 1, 0, 2, 0, 1, 2, 2, 1, 1, 1, 0, 1, 2, 1, 2, 2, 0, 1, 1, 0, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 1, 1, 1, 0, 2, 0, 1, 2, 0, 1, 1, 2, 1, 0, 0, 1, 2, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 0, 2, 1, 0, 2, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 2, 1, 1, 0, 2, 0, 1, 1, 1, 1, 2, 0, 1, 0, 2, 0, 0, 2, 0, 2, 1, 1, 2, 0, 2, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1, 1, 1, 0, 2, 1, 1, 0, 2, 2, 2, 2, 2, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 2, 0, 1, 0, 1, 2, 0, 0, 2, 1, 0, 2, 2, 1, 1, 1, 0, 2, 0, 0, 2, 2, 1, 1, 1, 1, 1, 0, 0, 1, 2, 2, 0, 1, 1, 0, 0, 2, 0, 2, 1, 1, 1, 1, 2, 1, 0, 1, 0, 2, 1, 1, 1, 0, 0, 2, 1, 1, 1, 2, 1, 1, 0, 1, 2, 2, 1, 0, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 0, 0, 1, 0, 0, 2, 1, 2, 0, 1, 2, 1, 2, 0, 1, 1, 2, 0, 2, 1, 1, 2, 2, 0, 2, 0, 2, 0, 2, 1, 2, 0, 2, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 0, 0, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 2, 2, 0, 0]\n",
      "actual_class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                47                67                       54\n",
      "1                               100                92                       69\n",
      "2                                63                47                       61\n",
      "Cluster 1 : Topic Disaster and Accident\n",
      "Cluster 1 : Topic News and Economy\n",
      "Cluster 1 : Topic Travel & Transportation\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.38      0.48      0.42       210\n",
      "       News and Economy       0.27      0.23      0.25       206\n",
      "Travel & Transportation       0.32      0.29      0.31       184\n",
      "\n",
      "               accuracy                           0.34       600\n",
      "              macro avg       0.33      0.33      0.33       600\n",
      "           weighted avg       0.33      0.34      0.33       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q2\n",
    "def cluster_lda(train_file, test_file):\n",
    "    \n",
    "    train=pd.read_json(train_file)\n",
    "    test=pd.read_json(test_file)\n",
    "    test_label = [test[1][x][0] for x in range(len(test))]\n",
    "    topic_assign = None\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.90, \\\n",
    "                min_df=5, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(train[0])\n",
    "\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    \n",
    "    num_topics = 3\n",
    "#    lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "#                                max_iter=25,verbose=1,\n",
    "#                                evaluate_every=1, n_jobs=1,\n",
    "#                                random_state=0).fit(tf)\n",
    "#    topic_assign=lda.transform(tf)\n",
    "\n",
    "            \n",
    "    corpus = gensim.matutils.Sparse2Corpus(tf, \\\n",
    "                            documents_columns=False)\n",
    "\n",
    "    id2word={idx:w for idx, w in \\\n",
    "    enumerate(tf_vectorizer.get_feature_names())}\n",
    "    \n",
    "    dictionary = corpora.Dictionary.from_corpus(corpus, \\\n",
    "                id2word=id2word)\n",
    "    ldamodel = gensim.models.\\\n",
    "    ldamodel.LdaModel(corpus, alpha='asymmetric',\\\n",
    "                            num_topics = num_topics, \\\n",
    "                            id2word=id2word, \\\n",
    "                            iterations=20)\n",
    "    tf_test = tf_vectorizer.fit_transform(test[0])\n",
    "\n",
    "    \n",
    "    test_corpus = gensim.matutils.Sparse2Corpus(tf_test, \\\n",
    "                    documents_columns=False)\n",
    "    predict = ldamodel.get_document_topics(test_corpus)\n",
    "    pred_val = []\n",
    "    for i in range(len(list(predict))):\n",
    "        val = max(list(predict)[i],key = lambda item:item[1])\n",
    "        pred_val.append(val[0])\n",
    "    print(pred_val)\n",
    "    \n",
    "    \n",
    "    ########## PRINT ############\n",
    "    \n",
    "    confusion_df_cs = pd.DataFrame(list(zip(test_label, pred_val)),\\\n",
    "                            columns = [\"actual_class\", \"cluster\"])\n",
    "    crosstab_cs = pd.crosstab( index=confusion_df_cs.cluster, columns=confusion_df_cs.actual_class)\n",
    "    cs_idx = crosstab_cs.idxmax(axis = 0)\n",
    "    cs_idx = cs_idx.sort_values(ascending=True) \n",
    "    \n",
    "    cluster_dict={0:'Travel & Transportation',\\\n",
    "              1:\"Disaster and Accident\",\\\n",
    "              2:'News and Economy'}\n",
    "    predicted_target=[cluster_dict[i] \\\n",
    "                  for i in pred_val]\n",
    "    print(crosstab_cs)\n",
    "    for index, val in cs_idx.iteritems():\n",
    "        print(\"Cluster\",val,\": Topic\",index)  \n",
    "    print(metrics.classification_report\\\n",
    "      (test_label, predicted_target))\n",
    "\n",
    "def find_similar(doc_id, topic_assign):\n",
    "    \n",
    "    docs = None\n",
    "        \n",
    "    return docs\n",
    "cluster_lda('/Users/parthxparab/Documents/Fall 2019/BIA660/Lab5/Assignment/train_text.json', \\\n",
    "             '/Users/parthxparab/Documents/Fall 2019/BIA660/Lab5/Assignment/test_text.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "print(\"\\nQ2\")\n",
    "topic_assign =cluster_lda('/Users/parthxparab/Documents/Fall 2019/BIA660/Lab5/Assignment/train_text.json', \\\n",
    "             '/Users/parthxparab/Documents/Fall 2019/BIA660/Lab5/Assignment/test_text.json')\n",
    "doc_ids = find_similar(10, topic_assign)\n",
    "print (\"docs similar to {0}: {1}\".format(10, doc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
