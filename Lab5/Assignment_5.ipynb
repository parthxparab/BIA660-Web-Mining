{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Clustering and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll need to use the following dataset:\n",
    "- text_train.json: This file contains a list of documents. It's used for training models\n",
    "- text_test.json: This file contains a list of documents and their ground-truth labels. It's used for testing performance. This file is in the format shown below. Note, each document has a list of labels.\n",
    "You can load these files using json.load()\n",
    "\n",
    "|Text| Labels|\n",
    "|----|-------|\n",
    "|paraglider collides with hot air balloon ... | ['Disaster and Accident', 'Travel & Transportation']|\n",
    "|faa issues fire warning for lithium ... | ['Travel & Transportation'] |\n",
    "| .... |...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: K-Mean Clustering\n",
    "\n",
    "Define a function **cluster_kmean()** as follows: \n",
    "- Take two file name strings as inputs: $train\\_file$ is the file path of text_train.json, and $test\\_file$ is the file path of text_test.json\n",
    "- When generating tfidf weights, set the min_df to 5.\n",
    "- Use **KMeans** to cluster documents in $train\\_file$ into 3 clusters by **cosine similarity**  and **Euclidean distance** separately. Use sufficient iterations with different initial centroids to make sure clustering converge \n",
    "- Test the clustering model performance using $test\\_file$: \n",
    "  * Predict the cluster ID for each document in $test\\_file$.\n",
    "  * Let's only use the **first label** in the ground-truth label list of each test document, e.g. for the first document in the table above, you set the ground_truth label to \"Disaster and Accident\" only.\n",
    "  * Apply **majority vote** rule to dynamically map the predicted cluster IDs to the ground-truth labels in $test\\_file$. **Be sure not to hardcode the mapping** (e.g. write code like {0: \"Disaster and Accident\"}), because a  cluster may corrspond to a different topic in each run. (hint: if you use pandas, look for \"idxmax\" function) \n",
    "  * Calculate **precision/recall/f-score** for each label, compare the results from the two clustering models, and write your analysis in a pdf file \n",
    "- This function has no return. Print out confusion matrix, precision/recall/f-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: LDA Clustering \n",
    "\n",
    "Q2.1. Define a function **cluster_lda()** as follows: \n",
    "1. Take two file name strings as inputs: $train\\_file$ is the file path of text_train.json, and $test\\_file$ is the file path of text_test.json\n",
    "2. Use **LDA** to train a topic model with documents in $train\\_file$ and the number of topics $K$ = 3. Keep min_df to 5 when generating tfidf weights, as in Q1.  \n",
    "3. Predict the topic distribution of each document in  $test\\_file$ and select the topic with highest probability. Similar to Q1, apply **majority vote rule** to map the topics to the labels and show the classification report. \n",
    "4. Return the array of topic proportion array\n",
    "\n",
    "Q2.2. Find similar documents\n",
    "- Define a function **find_similar_doc(doc_id, topic_mix)** to find **top 3 documents** that are the most similar to a selected one with index **doc_id** using the topic proportion array **topic_mix**. \n",
    "- You can calculate the cosine or Euclidean distance between two documents using the topic proportion array\n",
    "- Return the IDs of these similar documents.\n",
    "\n",
    "Q2.3. Provide a pdf document which contains: \n",
    "  - performance comparison between Q1 and Q2.1\n",
    "  - describe how you tune the model parameters, e.g. alpha, max_iter etc. in Q2.1.\n",
    "  - discuss how effective the method in Q2.2 is to find similar documents, compared with the tfidf weight cosine similarity we used before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 (Bonus): Biterm Topic Model (BTM)\n",
    "- There are many variants of LDA model. BTM is one designed for short text, while lDA in general expects documents with rich content.\n",
    "- Read this paper carefully http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.4032&rep=rep1&type=pdf and try to understand the design\n",
    "- Try the following experiments:\n",
    "    - Script a few thousand tweets by different hastags\n",
    "    - Run LDA and BTM respectively to discover topics among the collected tweets. BTM package can be found at https://pypi.org/project/biterm/\n",
    "    - Compare the performance of each model. If one model works better, explain why it works better,\n",
    "- Summarize your experiment in a pdf document.\n",
    "- Note there is no absolute right or wrong answer in this experiment. All you need is to give a try and understand how BTM works and differences between BTM and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Due to randomness involved in these alogorithms, you may get the same result as what I showed below. However, your result should be close after you tune parameters carefully.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster import KMeansClusterer, \\\n",
    "cosine_distance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q1    \n",
    "def cluster_kmean(train_file, test_file):\n",
    "    # add your code here\n",
    "    train=pd.read_json(train_file)\n",
    "    test=pd.read_json(test_file)\n",
    "    test_label = [test[1][x][0] for x in range(len(test))]\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(stop_words=\"english\",\\\n",
    "                             min_df=5) \n",
    "\n",
    "    dtm= tfidf_vect.fit_transform(train[0])\n",
    "    \n",
    "    num_clusters=3\n",
    "    \n",
    "    #### COSINE ####\n",
    "\n",
    "    clusterer = KMeansClusterer(num_clusters, \\\n",
    "                        cosine_distance, \\\n",
    "                            repeats=25)\n",
    "\n",
    "    clusters_cosine = clusterer.cluster(dtm.toarray(), \\\n",
    "                             assign_clusters=True)\n",
    "    test_dtm = tfidf_vect.transform(test[0])\n",
    "\n",
    "    predicted_cosine = [clusterer.classify(v) for v in test_dtm.toarray()]    \n",
    "    \n",
    "    confusion_df_cs = pd.DataFrame(list(zip(test_label, predicted_cosine)),\\\n",
    "                            columns = [\"actual_class\", \"cluster\"])\n",
    "\n",
    "    crosstab_cs = pd.crosstab( index=confusion_df_cs.cluster, columns=confusion_df_cs.actual_class)\n",
    "    cs_idx = crosstab_cs.idxmax(axis = 0)\n",
    "    cs_idx = cs_idx.sort_values(ascending=True) \n",
    "    \n",
    "    cluster_dict={0:'Travel & Transportation',\\\n",
    "              1:\"Disaster and Accident\",\\\n",
    "              2:'News and Economy'}\n",
    "    predicted_target=[cluster_dict[i] \\\n",
    "                  for i in predicted_cosine]\n",
    "    \n",
    "    print(\"cosine\")\n",
    "    print(crosstab_cs)\n",
    "    for index, val in cs_idx.iteritems():\n",
    "        print(\"Cluster\",val,\": Topic\",index)  \n",
    "    print(metrics.classification_report\\\n",
    "      (test_label, predicted_target))\n",
    "\n",
    "\n",
    "    ###### EUCLIDEAN ######\n",
    "    \n",
    "    km = KMeans(n_clusters=num_clusters, n_init=25).fit(dtm)\n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    test_dtm = tfidf_vect.transform(test[0])\n",
    "\n",
    "    predicted_euclidean = km.predict(test_dtm)\n",
    "    \n",
    "    confusion_df_eu = pd.DataFrame(list(zip(test_label, predicted_euclidean)),\\\n",
    "                            columns = [\"actual_class\", \"cluster\"])\n",
    "    crosstab_eu = pd.crosstab( index=confusion_df_eu.cluster, columns=confusion_df_eu.actual_class)\n",
    "    eu_idx = crosstab_eu.idxmax(axis = 0)\n",
    "    \n",
    "    eu_idx = eu_idx.sort_values(ascending=True)\n",
    "    cluster_dict={1:'Travel & Transportation',\\\n",
    "              0:\"Disaster and Accident\",\\\n",
    "              2:'News and Economy'}\n",
    "    predicted_target=[cluster_dict[i] \\\n",
    "                  for i in predicted_euclidean]\n",
    "\n",
    "    print(\"L2\")\n",
    "    print(crosstab_eu)\n",
    "    for index, val in eu_idx.iteritems():\n",
    "        print(\"Cluster\",val,\": Topic\",index)\n",
    "    print(metrics.classification_report\\\n",
    "      (test_label, predicted_target))\n",
    "\n",
    "\n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q2\n",
    "def cluster_lda(train_file, test_file):\n",
    "    \n",
    "    train=pd.read_json(train_file)\n",
    "    test=pd.read_json(test_file)\n",
    "    test_label = [test[1][x][0] for x in range(len(test))]\n",
    "    topic_assign = None\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.90, \\\n",
    "                min_df=5, stop_words='english')\n",
    "    tf_test = tf_vectorizer.fit_transform(test[0])\n",
    "\n",
    "    tf = tf_vectorizer.fit_transform(train[0])\n",
    "\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    \n",
    "    num_topics = 3\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "                                max_iter=25,verbose=1,\n",
    "                                evaluate_every=1, n_jobs=1,\n",
    "                                random_state=0).fit(tf)\n",
    "    topic_assign=lda.transform(tf)\n",
    "    \n",
    "    corpus = gensim.matutils.Sparse2Corpus(tf, \\\n",
    "                            documents_columns=False)\n",
    "\n",
    "    id2word={idx:w for idx, w in \\\n",
    "    enumerate(tf_vectorizer.get_feature_names())}\n",
    "    \n",
    "    dictionary = corpora.Dictionary.from_corpus(corpus, \\\n",
    "                id2word=id2word)\n",
    "    ldamodel = gensim.models.\\\n",
    "    ldamodel.LdaModel(corpus, alpha='asymmetric',\\\n",
    "                            num_topics = num_topics, \\\n",
    "                            id2word=id2word, \\\n",
    "                            iterations=25)\n",
    "    \n",
    "    test_corpus = gensim.matutils.Sparse2Corpus(tf_test, \\\n",
    "                    documents_columns=False)\n",
    "    predict = ldamodel.get_document_topics(test_corpus)\n",
    "    pred_val = []\n",
    "    for i in range(len(list(predict))):\n",
    "        val = max(list(predict)[i],key = lambda item:item[1])\n",
    "        pred_val.append(val[0])\n",
    "    \n",
    "    \n",
    "    ########## PRINT ############\n",
    "    \n",
    "    confusion_df_cs = pd.DataFrame(list(zip(test_label, pred_val)),\\\n",
    "                            columns = [\"actual_class\", \"cluster\"])\n",
    "    crosstab_cs = pd.crosstab( index=confusion_df_cs.cluster, columns=confusion_df_cs.actual_class)\n",
    "    cs_idx = crosstab_cs.idxmax(axis = 0)\n",
    "    cs_idx = cs_idx.sort_values(ascending=True) \n",
    "    \n",
    "    cluster_dict={0:'Travel & Transportation',\\\n",
    "              1:\"Disaster and Accident\",\\\n",
    "              2:'News and Economy'}\n",
    "    predicted_target=[cluster_dict[i] \\\n",
    "                  for i in pred_val]\n",
    "    print(crosstab_cs)\n",
    "    for index, val in cs_idx.iteritems():\n",
    "        print(\"Cluster\",val,\": Topic\",index)  \n",
    "    print(metrics.classification_report\\\n",
    "      (test_label, predicted_target))\n",
    "    \n",
    "    return topic_assign\n",
    "\n",
    "\n",
    "def find_similar(doc_id, topic_assign):\n",
    "    \n",
    "    docs = None\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1\n",
      "cosine\n",
      "actual_class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                61                 2                      152\n",
      "1                               109                 7                       25\n",
      "2                                40               197                        7\n",
      "Cluster 0: Topic Travel & Transportation\n",
      "Cluster 1: Topic Disaster and Accident\n",
      "Cluster 2: Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.77      0.52      0.62       210\n",
      "       News and Economy       0.81      0.96      0.88       206\n",
      "Travel & Transportation       0.71      0.83      0.76       184\n",
      "\n",
      "              micro avg       0.76      0.76      0.76       600\n",
      "              macro avg       0.76      0.77      0.75       600\n",
      "           weighted avg       0.76      0.76      0.75       600\n",
      "\n",
      "L2\n",
      "actual_class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                               174                34                      174\n",
      "1                                31               166                       10\n",
      "2                                 5                 6                        0\n",
      "Cluster 0: Topic Disaster and Accident\n",
      "Cluster 1: Topic News and Economy\n",
      "Cluster 2: Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.46      0.83      0.59       210\n",
      "       News and Economy       0.79      0.83      0.81       206\n",
      "Travel & Transportation       0.00      0.00      0.00       184\n",
      "\n",
      "              micro avg       0.58      0.58      0.58       600\n",
      "              macro avg       0.41      0.55      0.47       600\n",
      "           weighted avg       0.43      0.58      0.48       600\n",
      "\n",
      "\n",
      "Q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rliu/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 25\n",
      "iteration: 2 of max_iter: 25\n",
      "iteration: 3 of max_iter: 25\n",
      "iteration: 4 of max_iter: 25\n",
      "iteration: 5 of max_iter: 25, perplexity: 3494.8408\n",
      "iteration: 6 of max_iter: 25\n",
      "iteration: 7 of max_iter: 25\n",
      "iteration: 8 of max_iter: 25\n",
      "iteration: 9 of max_iter: 25\n",
      "iteration: 10 of max_iter: 25, perplexity: 3416.5917\n",
      "iteration: 11 of max_iter: 25\n",
      "iteration: 12 of max_iter: 25\n",
      "iteration: 13 of max_iter: 25\n",
      "iteration: 14 of max_iter: 25\n",
      "iteration: 15 of max_iter: 25, perplexity: 3382.7160\n",
      "iteration: 16 of max_iter: 25\n",
      "iteration: 17 of max_iter: 25\n",
      "iteration: 18 of max_iter: 25\n",
      "iteration: 19 of max_iter: 25\n",
      "iteration: 20 of max_iter: 25, perplexity: 3377.7126\n",
      "iteration: 21 of max_iter: 25\n",
      "iteration: 22 of max_iter: 25\n",
      "iteration: 23 of max_iter: 25\n",
      "iteration: 24 of max_iter: 25\n",
      "iteration: 25 of max_iter: 25, perplexity: 3375.9923\n",
      "actual_class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                30                18                      138\n",
      "1                                12               182                        8\n",
      "2                               168                 6                       38\n",
      "Cluster 0: Topic Travel & Transportation\n",
      "Cluster 1: Topic News and Economy\n",
      "Cluster 2: Topic Disaster and Accident\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.79      0.80      0.80       210\n",
      "       News and Economy       0.90      0.88      0.89       206\n",
      "Travel & Transportation       0.74      0.75      0.75       184\n",
      "\n",
      "              micro avg       0.81      0.81      0.81       600\n",
      "              macro avg       0.81      0.81      0.81       600\n",
      "           weighted avg       0.81      0.81      0.81       600\n",
      "\n",
      "cluster\n",
      "0    Travel & Transportation\n",
      "1           News and Economy\n",
      "2      Disaster and Accident\n",
      "dtype: object\n",
      "docs similar to 10: [337  38 222]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Due to randomness, you won't get the exact result\n",
    "    # as shown here, but your result should be close\n",
    "    # if you tune the parameters carefully\n",
    "    \n",
    "    # Q1\n",
    "    print(\"Q1\")\n",
    "    cluster_kmean('../../dataset/train_text.json', \\\n",
    "                  '../../dataset/test_text.json')\n",
    "            \n",
    "    # Q2\n",
    "    print(\"\\nQ2\")\n",
    "    topic_assign =cluster_lda('../../dataset/train_text.json', \\\n",
    "        '../../dataset/test_text.json')\n",
    "    doc_ids = find_similar(10, topic_assign)\n",
    "    print (\"docs similar to {0}: {1}\".format(10, doc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
