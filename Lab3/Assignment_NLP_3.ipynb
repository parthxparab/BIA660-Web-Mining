{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment 3 : Text Processing </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Regular Expression\n",
    "Define a function \"**tokenize**\" as follows: \n",
    "   - takes a string as an input\n",
    "   - converts the string into lower case\n",
    "   - tokenizes the lower-cased string into tokens. A token is defined as follows:\n",
    "      - a token has at least 2 characters\n",
    "      - a token must start with an alphabetic letter (i.e. a-z or A-Z), \n",
    "      - a token can have alphabetic letters, \"\\-\" (hyphen), \".\" (dot), \"'\" (single quote), or \"\\_\" (underscore) in the middle\n",
    "      - a token must end with an alphabetic letter (i.e. a-z or A-Z) \n",
    "   - removes stop words from the tokens (use English stop words list from NLTK) \n",
    "   - returns the resulting token list as the output\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Sentimeent Analysis\n",
    "1. First define a function \"**sentiment_analysis**\" as follows: \n",
    "  - takes a string, a list of positive words, and a list of negative words as inputs. Assume the lists are read from positive-words.txt and negative-words.txt outside of this function.\n",
    "  - tokenizes the string using the tokeniz function defined above \n",
    "  - counts positive words and negative words in the tokens using the positive/negative words lists. With a list of negation words (i.e. not, no, isn't, wasn't, aren't, weren't, don't didn't, cannot, couldn't, won't, neither, nor), the final positive/negative words are defined as follows:\n",
    "    - Positive words:\n",
    "      * a positive word not preceded by a negation word \n",
    "      * a negative word preceded by a negation word\n",
    "    - Negative words:\n",
    "      * a negative word not preceded by a negation word\n",
    "      * a positive word preceded by a negation word\n",
    "    - determined the sentiment of the string as follows:\n",
    "       - 2: number of positive words > number of negative words\n",
    "       - 1: number of positive words <= number of negative words\n",
    "  - returns the sentiment\n",
    "    \n",
    "2. Define a function called **performance_evaluate** to evaluate the accuracy of the sentiment analysis in (1) as follows:  \n",
    "   - takes an input file (\"amazon_review_300.csv\"), a list of positive words, and a list of negative words as inputs. The input file has a list of reviews in the format of (label, review).\n",
    "   - reads the input file to get a list of reviews including review text and label of each review \n",
    "   - for each review, predicts its sentiment using the function defined in \n",
    "   - returns the accuracy as the number of correct sentiment predictions/total reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: (Bonus) Vector Space Model\n",
    "\n",
    "1. Define a function **find_similar_doc** as follows: \n",
    "    - takes two inputs: a list of documents (i.e. docs), and the index of a selected document as an integer (i.e. doc_id).\n",
    "    - uses the \"tokenize\" function defined in Q1 to tokenize each document \n",
    "    - generates normalized tf_idf matrix (each row is normalized) from the tokens (hint: reference to the tf_idf function defined in Section 8.5 in lecture notes) \n",
    "    - calculates the pairwise cosine distance of documents using the generated tf_idf matrix \n",
    "    - for the selected doc_id, finds the index of the most similar document (but not itself) by the cosine similarity score \n",
    "    - returns the index of the most similar document and the similarity score\n",
    "2. Test your function with \"amazon_review_300.csv\" and a few reviews from this file.\n",
    "   - Check the most similar review discovered for each of the selected reviews\n",
    "   - Can you use the calculated similarity score to determine if two documents are similar?  \n",
    "   - Do you think this function can successfully find similar documents? Why does it work or not work? \n",
    "   - If it does not work, what can you do to improve the search?\n",
    "   - Write down your analysis along with some evidence or observations you have in a pdf file and submit this pdf file along with your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    tokens = []\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    text=text.lower()\n",
    "    \n",
    "    text= re.sub(r'\\b(.+)(\\s+\\1\\b)+', r'\\1', text)\n",
    "    tokens= re.findall(r'\\w+\\S\\w+', text)\n",
    "    \n",
    "    tokens = [w for w in tokens if not w in stop_words] \n",
    "    \n",
    "    return(tokens) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text, positive_words, negative_words):\n",
    "    \n",
    "    negations=[\"not\", \"no\", \"isn't\", \"wasn't\", \"aren't\", \\\n",
    "               \"weren't\", \"don't\", \"didn't\", \"cannot\", \\\n",
    "               \"couldn't\", \"won't\", \"neither\", \"nor\"]\n",
    "    tokens=tokenize(text)\n",
    "    \n",
    "    sentiment = None\n",
    "    \n",
    "    # add your code\n",
    "#    positive_tokens=[]\n",
    "#    negative_tokens=[]\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token in positive_words:\n",
    "            if idx>0:\n",
    "                if tokens[idx-1] not in negations:\n",
    "           #         positive_tokens.append(token)\n",
    "                    positive = positive + 1\n",
    "                elif tokens[idx-1] in negations:\n",
    "           #         negative_tokens.append(token)\n",
    "                    negative = negative + 1\n",
    "            else:\n",
    "           #     positive_tokens.append(token)\n",
    "                positive = positive + 1\n",
    "                \n",
    "        if token in negative_words:\n",
    "            if idx>0:\n",
    "                if tokens[idx-1] not in negations:\n",
    "           #         negative_tokens.append(token)\n",
    "                    negative = negative + 1\n",
    "                elif tokens[idx-1] in negations:\n",
    "           #         positive_tokens.append(token)\n",
    "                    positive = positive + 1\n",
    "            else:\n",
    "           #     negative_tokens.append(token)\n",
    "                negative = negative + 1        \n",
    "                \n",
    "    if positive > negative:\n",
    "        sentiment = 2\n",
    "    elif positive <= negative:\n",
    "        sentiment = 1    \n",
    "    return sentiment\n",
    "\n",
    "\n",
    "def performance_evaluate(input_file, positive_words, negative_words):\n",
    "    \n",
    "    accuracy = None\n",
    "    count = 0\n",
    "    value = 0\n",
    "    \n",
    "    # add your code\n",
    "    with open(input_file,'r') as f:\n",
    "        input_file=[line.strip() for line in f]  \n",
    "    for i in range(1, len(input_file)):\n",
    "        value = str(sentiment_analysis(input_file[i],positive_words, negative_words))\n",
    "        if value == input_file[i][0]:\n",
    "            count = count + 1\n",
    "    accuracy = count/len(input_file)\n",
    "    \n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 tokens: ['composed', 'cds', 'quite', 'songs', 'exact', 'count', 'heart-rendering', 'impressively', 'remarkable', 'everything', 'every', 'listener', 'fast-paced', 'energetic', 'dancing', 'tokage', 'termina', 'home', 'slower', 'haunting', 'dragon', 'god', 'purely', 'beautifully', 'composed', \"time's\", 'scar', 'even', 'fantastic', 'vocals', 'radical', 'dreamers', 'one', 'best', 'videogame', 'soundtracks', 'surely', \"mitsuda's\", 'best', 'ever']\n",
      "214\n",
      "301\n",
      "\n",
      "Q2 accuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Q1\n",
    "    text=\"Composed of 3 CDs and quite a few songs (I haven't an exact count), \\\n",
    "          all of which are heart-rendering and impressively remarkable. \\\n",
    "          It has everything for every listener -- from fast-paced and energetic \\\n",
    "          (Dancing the Tokage or Termina Home), to slower and more haunting (Dragon God), \\\n",
    "          to purely beautifully composed (Time's Scar), \\\n",
    "          to even some fantastic vocals (Radical Dreamers).\\\n",
    "          This is one of the best videogame soundtracks out there, \\\n",
    "          and surely Mitsuda's best ever. ^_^\"\n",
    "\n",
    "    tokens=tokenize(text)\n",
    "    \n",
    "    print(\"Q1 tokens:\", tokens)\n",
    "    \n",
    "    # Test Q2\n",
    "    \n",
    "    with open(\"./dataset/positive-words.txt\",'r') as f:\n",
    "        positive_words=[line.strip() for line in f]\n",
    "        \n",
    "    with open(\"./dataset/negative-words.txt\",'r') as f:\n",
    "        negative_words=[line.strip() for line in f]\n",
    "        \n",
    "    acc=performance_evaluate(\"./dataset/amazon_review_300.csv\", \\\n",
    "                                  positive_words, negative_words)\n",
    "    print(\"\\nQ2 accuracy: {0:.2f}\".format(acc))\n",
    "   \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
